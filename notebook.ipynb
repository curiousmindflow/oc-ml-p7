{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, re\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score, hamming_loss, precision_score, recall_score, f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import scripts\n",
    "\n",
    "def set_seed(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed()\n",
    "\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "if tf.test.gpu_device_name():\n",
    "    print(f\"GPU used: {tf.test.gpu_device_name()}\")\n",
    "else:\n",
    "    print(f\"GPU not used\")\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"raw_preprocessing\": False,\n",
    "    \"preprocessing\": False,\n",
    "    \"baseline\": False,\n",
    "    \"bert_base\": False,\n",
    "    \"bert_se\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_raw_data_body(cell):\n",
    "    soup = bs(cell, \"html.parser\")\n",
    "\n",
    "    script_tags = soup.find_all(\"script\")\n",
    "    for script_tag in script_tags:\n",
    "        script_tag.extract()\n",
    "\n",
    "    code_tags = soup.find_all(\"code\")\n",
    "    for code_tag in code_tags:\n",
    "        code_tag.extract()\n",
    "\n",
    "    preproc_cell = soup.get_text()\n",
    "    preproc_cell = preproc_cell.replace(',', ' ')\n",
    "\n",
    "    return preproc_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_list(cell):\n",
    "    return [tag for tag in re.split(r'[<>]', cell) if tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(cell):\n",
    "    return cell.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(*texts):\n",
    "    tokens = []\n",
    "    for text in texts:\n",
    "        # https://regex101.com/\n",
    "        tokenizer = nltk.RegexpTokenizer(r'\\.?[a-z#]+')\n",
    "        tokens_temp = tokenizer.tokenize(text)\n",
    "        tokens += [re.sub(\"(.)\\\\1{3,}\", \"\\\\1\", token) for token in tokens_temp]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(cell):\n",
    "    return [word for word in cell if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_pos(cell):\n",
    "    treebank_tags = pos_tag(cell)\n",
    "    pos = [(tag[0], get_wordnet_pos(tag[1])) for tag in treebank_tags]\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(cell, with_pos=False):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    if not with_pos:\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in cell]\n",
    "    else:\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(pair[0], pos=pair[1]) for pair in cell]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmize(cell):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in cell]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data_body = pd.read_csv(\"data/raw_data_body.csv\")\n",
    "    raw_data_body.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Parse html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data_body[\"Body\"] = raw_data_body.apply(lambda row: preproc_raw_data_body(row.Body), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Merge body with rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data_rest = pd.read_csv(\"data/raw_data_id_title_tags.csv\")\n",
    "    raw_data = raw_data_rest.join(raw_data_body)\n",
    "    raw_data = raw_data.set_index(\"Id\").reindex([\"Title\", \"Body\", \"Tags\"], axis=\"columns\")\n",
    "    display = raw_data.head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Tags to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data[\"Tags\"] = raw_data.apply(lambda row: tags_to_list(row[\"Tags\"]), axis=\"columns\")\n",
    "    display = raw_data[[\"Tags\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Lowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data[\"Title\"] = raw_data.apply(lambda row: lower(row[\"Title\"]), axis=\"columns\")\n",
    "    raw_data[\"Body\"] = raw_data.apply(lambda row: lower(row[\"Body\"]), axis=\"columns\")\n",
    "    display = raw_data[[\"Title\", \"Body\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Save raw preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"raw_preprocessing\"]:\n",
    "    \n",
    "    raw_data.to_csv(\"data/raw_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Load raw preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"preprocessing\"]:\n",
    "    \n",
    "    data = pd.read_csv(\"data/raw_data.csv\", index_col=\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"Tokens\"] = data.apply(lambda row: tokenize(row[\"Body\"], row[\"Title\"]), axis=\"columns\")\n",
    "    display = data[[\"Title\", \"Body\", \"Tokens\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 StopWords deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    data[\"Tokens\"] = data.apply(lambda row: remove_stop_words(row[\"Tokens\"]), axis=\"columns\")\n",
    "    display = data[[\"Title\", \"Body\", \"Tokens\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 POS - Part-Of-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"POS\"] = data.apply(lambda row: tag_pos(row[\"Tokens\"]), axis=\"columns\")\n",
    "    display = data[[\"Tokens\", \"POS\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"Lemmatized\"] = data.apply(lambda row: lemmatize(row[\"POS\"], with_pos=True), axis=\"columns\")\n",
    "    display = data[[\"Tokens\", \"Lemmatized\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 Stemmize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"LemmaAndStem\"] = data.apply(lambda row: stemmize(row[\"Tokens\"]), axis=\"columns\")\n",
    "    display = data[[\"Tokens\", \"LemmaAndStem\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13 Generating sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"Sentence\"] = data.apply(lambda row: \" \".join([str(item) for item in row[\"LemmaAndStem\"]]), axis=\"columns\")\n",
    "    display = data[[\"LemmaAndStem\", \"Sentence\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.14 Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data.to_csv(\"data/data_cleaned.csv\", index_label=\"Id\")\n",
    "    display = data.head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(dataset, max_features=None, min_df=0.0, max_df=1.0):\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features=max_features, max_df=max_df, min_df=min_df)\n",
    "    matrix = vectorizer.fit_transform(dataset)\n",
    "\n",
    "    data_dense = matrix.todense()\n",
    "    print(f\"Sparcity: {((data_dense > 0).sum() / data_dense.size)*100:.4}%\")\n",
    "\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "    matrix = matrix.toarray()\n",
    "    bag = pd.DataFrame(data=matrix, columns=vocab)\n",
    "    return bag, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(dataset, max_features=None, min_df=0.0, max_df=1.0):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=None, stop_words=None, max_features=max_features, min_df=min_df, max_df=max_df)\n",
    "    matrix = vectorizer.fit_transform(dataset).toarray()\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    tfidf = pd.DataFrame(data=matrix, columns=vocab)\n",
    "    return tfidf, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data_cleaned.csv\", index_col=\"Id\")\n",
    "\n",
    "data[\"Tags\"] = data[\"Tags\"].apply(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"Tags\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Tags.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "for row in data.Tags.values:\n",
    "    tags += row\n",
    "tags_df = pd.DataFrame(data=tags, columns=[\"Tag\"]).value_counts().reset_index()\n",
    "tags_df.columns = [\"Tag\", \"Count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "sns.barplot(data=tags_df.iloc[:20], x=\"Tag\", y=\"Count\")\n",
    "\n",
    "plt.title(\"Tag count\", size=20)\n",
    "plt.xlabel(\"Tag\", size=16)\n",
    "plt.ylabel(\"Count\", size=16)\n",
    "plt.xticks(rotation=45, size=16, ha=\"right\")\n",
    "plt.yticks(size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "ax = sns.ecdfplot(data=tags_df, x=\"Count\", log_scale=True)\n",
    "\n",
    "plt.axhline(0.98, linestyle=\"--\", linewidth=1, color=\"r\")\n",
    "plt.axvline(100, linestyle=\"--\", linewidth=1, color=\"r\")\n",
    "\n",
    "plt.title(\"Cummulative coverage percentage\", size=20)\n",
    "plt.xlabel(\"Number of post\", size=16)\n",
    "plt.ylabel(\"Proportion\", size=16)\n",
    "plt.xticks(rotation=45, size=16, ha=\"right\")\n",
    "plt.yticks(size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = tags_df[:100]\n",
    "tags_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = dict(zip(tags_df.Tag, tags_df.Count))\n",
    "wordcloud = WordCloud(background_color=\"black\", width=1600, height=800).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags_df.Tag.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_or_remove(cell, word_list):\n",
    "    return [word for word in cell if word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tags_Reduced\"] = data.apply(lambda row: find_or_remove(row[\"Tags\"], tags),axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tags_Reduced\"].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 4 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/#9buildldamodelwithsklearn\n",
    "#\n",
    "def latent_dirichlet_allocation_tuning(dataset: pd.DataFrame, param_grid: dict):\n",
    "    data_bow, vectorizer = bow(dataset, min_df=.005)\n",
    "    feature_names = data_bow.columns\n",
    "\n",
    "    lda = LatentDirichletAllocation()\n",
    "    gs = GridSearchCV(lda, param_grid)\n",
    "    gs.fit(data_bow)\n",
    "\n",
    "    lda_model = gs.best_estimator_\n",
    "    lda_output = lda_model.transform(data_bow)\n",
    "    topic_names = [\"Topic\"+str(i) for i in range(lda_model.n_components)]\n",
    "\n",
    "    lda_output_dataframe = pd.DataFrame(np.round(lda_output, 2), columns=topic_names)\n",
    "\n",
    "    return gs, feature_names, data_bow, vectorizer, lda_output_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_topic(lda_model, data_bow, dataset_row_nb):\n",
    "    lda_output = lda_model.transform(data_bow)\n",
    "\n",
    "    topic_names = [\"Topic\"+str(i) for i in range(lda_model.n_components)]\n",
    "    doc_names = [\"Doc\"+str(i) for i in range(dataset_row_nb)]\n",
    "\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topic_names, index=doc_names)\n",
    "\n",
    "    dominant_topics = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic[\"Dominant_Topic\"] = dominant_topics\n",
    "\n",
    "    return df_document_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_distribution(dominant_topic_df):\n",
    "    distribution = dominant_topic_df[\"Dominant_Topic\"].value_counts().reset_index()\n",
    "    distribution.columns = [\"Dominant_Topic\", \"Count\"]\n",
    "\n",
    "    plt.figure(figsize=(15, 4))\n",
    "\n",
    "    sns.countplot(data=dominant_topic_df, x=\"Dominant_Topic\")\n",
    "\n",
    "    plt.title(\"Tag count\", size=20)\n",
    "    plt.xlabel(\"Tag\", size=16)\n",
    "    plt.ylabel(\"Count\", size=16)\n",
    "    plt.xticks(rotation=45, size=16, ha=\"right\")\n",
    "    plt.yticks(size=16)\n",
    "    plt.show()\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_words(lda_model, feature_names, n_words=20):\n",
    "    keywords = np.array(feature_names)\n",
    "    topic_keywords = []\n",
    "    for topic_weight in lda_model.components_:\n",
    "        topic_keyword_locs = (-topic_weight).argsort()[:n_words]\n",
    "        topic_keywords.append(feature_names.take(topic_keyword_locs))\n",
    "    \n",
    "    topic_keywords_df = pd.DataFrame(data=topic_keywords)\n",
    "    topic_keywords_df.columns = [\"Word\"+str(i) for i in range(topic_keywords_df.shape[1])]\n",
    "    topic_keywords_df.index = [\"Topic\"+str(i) for i in range(topic_keywords_df.shape[0])]\n",
    "    return topic_keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(lda_model, sentence, vectorizer, topic_keywords_dataset):\n",
    "    sentence = scripts.preprocess_sentence(sentence)\n",
    "    data_bow = vectorizer.transform([sentence])\n",
    "    topic_probability_score = lda_model.transform(data_bow)\n",
    "    topic = topic_keywords_dataset.iloc[np.argmax(topic_probability_score), :]\n",
    "    topic_name = topic.name\n",
    "    topic_words = topic.values.tolist()\n",
    "    return topic_name, topic_words, topic_probability_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_tuning_post_lda(dataset_X: pd.DataFrame, dataset_y: pd.DataFrame, meta_model, model, param_grid: dict, scoring: str = \"f1_micro\"):\n",
    "    start = datetime.now()\n",
    "\n",
    "    # target multi label binarizer\n",
    "    multi_label_binarizer = MultiLabelBinarizer()\n",
    "    y = multi_label_binarizer.fit_transform(dataset_y)\n",
    "\n",
    "    feature_names = dataset_X.columns\n",
    "    classes = multi_label_binarizer.classes_\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset_X, y, test_size = 0.33, random_state = 1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # multioutput or onevsrest ...\n",
    "    meta_model.fit(X_train, y_train)\n",
    "\n",
    "    # gridsearch tuning/fitting\n",
    "    gs = GridSearchCV(meta_model, param_grid, scoring=scoring, refit=True)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # advanced evaluation\n",
    "    best_model = gs.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    print(f\"classifier_tuning > Time taken to run this cell : {datetime.now() - start} \\n\")\n",
    "\n",
    "    return gs, classes, y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gs, classes, y_test, y_pred):\n",
    "    start = datetime.now()\n",
    "\n",
    "    print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Hamming loss \", hamming_loss(y_test, y_pred))\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average='micro')\n",
    "    recall = recall_score(y_test, y_pred, average='micro')\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(\"Macro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "    print(classification_report(y_test, y_pred, target_names=classes, zero_division=0))\n",
    "\n",
    "    print(f\"evaluate > Time taken to run this cell : {datetime.now() - start}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.1 Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    param_grid = {\n",
    "        \"n_components\": [10],\n",
    "        \"learning_decay\": [.7],\n",
    "        \"random_state\": [0],\n",
    "        \"n_jobs\": [10]\n",
    "    }\n",
    "\n",
    "    gs, feature_names, data_bow, vectorizer, lda_output_dataframe = latent_dirichlet_allocation_tuning(data[\"Sentence\"], param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    print(f\"Log likelihood: {gs.best_estimator_.score(data_bow)}\")\n",
    "    print(f\"Perplexity: {gs.best_estimator_.perplexity(data_bow)}\")\n",
    "    print(f\"Best params: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    dominant_topic_df = get_dominant_topic(gs.best_estimator_, data_bow, data.shape[0])\n",
    "    dominant_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    topic_distribution(dominant_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    topic_keywords_df = pd.DataFrame(data=gs.best_estimator_.components_, columns=feature_names, index=dominant_topic_df.columns[:-1])\n",
    "    topic_keywords_df.info()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"baseline\"]:\n",
    "\n",
    "    display = topic_keywords_df.iloc[:, :20]\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"baseline\"]:\n",
    "\n",
    "    topic_keywords_dataset = topic_words(lda_model=gs.best_estimator_, feature_names=feature_names, n_words=20)\n",
    "    display = topic_keywords_dataset\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    param_grid = {\n",
    "        \"estimator__solver\": [\"liblinear\"],\n",
    "        \"estimator__penalty\": [\"l1\"],\n",
    "        \"estimator__random_state\": [0],\n",
    "    }\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    meta_model = OneVsRestClassifier(model)\n",
    "    gs, classes, y_test, y_pred = classifier_tuning_post_lda(lda_output_dataframe, data[\"Tags_Reduced\"], meta_model, model, param_grid)\n",
    "    evaluate(gs, classes, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.2 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/lda_results_metrics.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/lda_results_topic_distrib_plot.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/lda_results_topic_words.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/logistic_classifier_results_01.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/logistic_classifier_results_02.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/logistic_classifier_results_03.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 5 BERT_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 6 BERT_SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 7 Conclusion"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "586ad1ed5c97141e2437e681efbf1ec0adcd17d830cf5af2ca3d2819e743e158"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
