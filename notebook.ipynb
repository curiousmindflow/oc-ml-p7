{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, re, shutil\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score, hamming_loss, precision_score, recall_score, f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.python.profiler import trace\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Embedding, AveragePooling1D, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, AUC\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import bert\n",
    "from transformers import TFBertModel, BertConfig, BertTokenizerFast\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "import scripts\n",
    "\n",
    "def set_seed(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed()\n",
    "\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "if tf.test.gpu_device_name():\n",
    "    print(f\"GPU used: {tf.test.gpu_device_name()}\")\n",
    "else:\n",
    "    print(f\"GPU not used\")\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"raw_preprocessing\": False,\n",
    "    \"preprocessing\": False,\n",
    "    \"baseline\": False,\n",
    "    \"bert_base\": False,\n",
    "    \"bert_se\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_raw_data_body(cell):\n",
    "    soup = bs(cell, \"html.parser\")\n",
    "\n",
    "    script_tags = soup.find_all(\"script\")\n",
    "    for script_tag in script_tags:\n",
    "        script_tag.extract()\n",
    "\n",
    "    code_tags = soup.find_all(\"code\")\n",
    "    for code_tag in code_tags:\n",
    "        code_tag.extract()\n",
    "\n",
    "    preproc_cell = soup.get_text()\n",
    "    preproc_cell = preproc_cell.replace(',', ' ')\n",
    "\n",
    "    return preproc_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_list(cell):\n",
    "    return [tag for tag in re.split(r'[<>]', cell) if tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(cell):\n",
    "    return cell.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(*texts):\n",
    "    tokens = []\n",
    "    for text in texts:\n",
    "        # https://regex101.com/\n",
    "        tokenizer = nltk.RegexpTokenizer(r'\\.?[a-z#]+')\n",
    "        tokens_temp = tokenizer.tokenize(text)\n",
    "        tokens += [re.sub(\"(.)\\\\1{3,}\", \"\\\\1\", token) for token in tokens_temp]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(cell):\n",
    "    return [word for word in cell if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_pos(cell):\n",
    "    treebank_tags = pos_tag(cell)\n",
    "    pos = [(tag[0], get_wordnet_pos(tag[1])) for tag in treebank_tags]\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(cell, with_pos=False):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    if not with_pos:\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in cell]\n",
    "    else:\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(pair[0], pos=pair[1]) for pair in cell]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmize(cell):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in cell]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data_body = pd.read_csv(\"data/raw_data_body.csv\")\n",
    "    raw_data_body.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Parse html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data_body[\"Body\"] = raw_data_body.apply(lambda row: preproc_raw_data_body(row.Body), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Merge body with rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data_rest = pd.read_csv(\"data/raw_data_id_title_tags.csv\")\n",
    "    raw_data = raw_data_rest.join(raw_data_body)\n",
    "    raw_data = raw_data.set_index(\"Id\").reindex([\"Title\", \"Body\", \"Tags\"], axis=\"columns\")\n",
    "    raw_data[\"Sentence_Pristine\"] = raw_data.apply(lambda row: row[\"Title\"] + \" \" + row[\"Body\"], axis=\"columns\")\n",
    "    display = raw_data.head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Tags to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data[\"Tags\"] = raw_data.apply(lambda row: tags_to_list(row[\"Tags\"]), axis=\"columns\")\n",
    "    display = raw_data[[\"Tags\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Lowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data[\"Sentence_Pristine\"] = raw_data.apply(lambda row: lower(row[\"Sentence_Pristine\"]), axis=\"columns\")\n",
    "    display = raw_data[[\"Sentence_Pristine\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Save raw preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"raw_preprocessing\"]:\n",
    "    \n",
    "    raw_data.drop(columns=[\"Title\", \"Body\"], inplace=True)\n",
    "    raw_data.to_csv(\"data/raw_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Load raw preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"preprocessing\"]:\n",
    "    \n",
    "    data = pd.read_csv(\"data/raw_data.csv\", index_col=\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"Tokens\"] = data.apply(lambda row: tokenize(row[\"Sentence_Pristine\"]), axis=\"columns\")\n",
    "    display = data[[\"Sentence_Pristine\", \"Tokens\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 StopWords deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    data[\"Tokens\"] = data.apply(lambda row: remove_stop_words(row[\"Tokens\"]), axis=\"columns\")\n",
    "    display = data[[\"Sentence_Pristine\", \"Tokens\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 POS - Part-Of-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"POS\"] = data.apply(lambda row: tag_pos(row[\"Tokens\"]), axis=\"columns\")\n",
    "    display = data[[\"Tokens\", \"POS\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"Lemmatized\"] = data.apply(lambda row: lemmatize(row[\"POS\"], with_pos=True), axis=\"columns\")\n",
    "    display = data[[\"Tokens\", \"Lemmatized\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 Stemmize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"LemmaAndStem\"] = data.apply(lambda row: stemmize(row[\"Tokens\"]), axis=\"columns\")\n",
    "    display = data[[\"Tokens\", \"LemmaAndStem\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13 Generating sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"Sentence\"] = data.apply(lambda row: \" \".join([str(item) for item in row[\"LemmaAndStem\"]]), axis=\"columns\")\n",
    "    display = data[[\"LemmaAndStem\", \"Sentence\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.14 Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data.to_csv(\"data/data_cleaned.csv\", index_label=\"Id\")\n",
    "    display = data.head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(dataset, max_features=None, min_df=0.0, max_df=1.0):\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features=max_features, max_df=max_df, min_df=min_df)\n",
    "    matrix = vectorizer.fit_transform(dataset)\n",
    "\n",
    "    data_dense = matrix.todense()\n",
    "    print(f\"Sparcity: {((data_dense > 0).sum() / data_dense.size)*100:.4}%\")\n",
    "\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "    matrix = matrix.toarray()\n",
    "    bag = pd.DataFrame(data=matrix, columns=vocab)\n",
    "    return bag, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(dataset, max_features=None, min_df=0.0, max_df=1.0):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=None, stop_words=None, max_features=max_features, min_df=min_df, max_df=max_df)\n",
    "    matrix = vectorizer.fit_transform(dataset).toarray()\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    tfidf = pd.DataFrame(data=matrix, columns=vocab)\n",
    "    return tfidf, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data_cleaned.csv\", index_col=\"Id\")\n",
    "data = data.reset_index()\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "data[\"Tags\"] = data[\"Tags\"].apply(eval)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"Sentence_Pristine\", \"Sentence\", \"Tags\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.2 Dataset constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT_SE fine_tuning has been done for a dataset of 23 313 rows, then our 50k rows dataset must be constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:23313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.3 Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"Tags\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Tags.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "for row in data.Tags.values:\n",
    "    tags += row\n",
    "tags_df = pd.DataFrame(data=tags, columns=[\"Tag\"]).value_counts().reset_index()\n",
    "tags_df.columns = [\"Tag\", \"Count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "sns.barplot(data=tags_df.iloc[:20], x=\"Tag\", y=\"Count\")\n",
    "\n",
    "plt.title(\"Tag count\", size=20)\n",
    "plt.xlabel(\"Tag\", size=16)\n",
    "plt.ylabel(\"Count\", size=16)\n",
    "plt.xticks(rotation=45, size=16, ha=\"right\")\n",
    "plt.yticks(size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "ax = sns.ecdfplot(data=tags_df, x=\"Count\", log_scale=True)\n",
    "\n",
    "plt.axhline(0.98, linestyle=\"--\", linewidth=1, color=\"r\")\n",
    "plt.axvline(30, linestyle=\"--\", linewidth=1, color=\"r\")\n",
    "\n",
    "plt.title(\"Cummulative coverage percentage\", size=20)\n",
    "plt.xlabel(\"Number of post\", size=16)\n",
    "plt.ylabel(\"Proportion\", size=16)\n",
    "plt.xticks(rotation=45, size=16, ha=\"right\")\n",
    "plt.yticks(size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = tags_df[:30]\n",
    "TAGS_NB = tags_df.shape[0]\n",
    "tags_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = dict(zip(tags_df.Tag, tags_df.Count))\n",
    "wordcloud = WordCloud(background_color=\"black\", width=1600, height=800).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags_df.Tag.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_or_remove(cell, word_list):\n",
    "    return [word for word in cell if word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tags_Reduced\"] = data.apply(lambda row: find_or_remove(row[\"Tags\"], tags),axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tags_Reduced\"].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tags_Reduced\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Tags_Reduced\"] = data.apply(lambda row: at_least_two(row), axis=1)\n",
    "# data.dropna(subset=[\"Tags_Reduced\"], inplace=True)\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 4 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/#9buildldamodelwithsklearn\n",
    "#\n",
    "def latent_dirichlet_allocation_tuning(dataset: pd.DataFrame, param_grid: dict):\n",
    "    data_bow, vectorizer = bow(dataset, min_df=.005)\n",
    "    feature_names = data_bow.columns\n",
    "\n",
    "    lda = LatentDirichletAllocation()\n",
    "    gs = GridSearchCV(lda, param_grid)\n",
    "    gs.fit(data_bow)\n",
    "\n",
    "    lda_model = gs.best_estimator_\n",
    "    lda_output = lda_model.transform(data_bow)\n",
    "    topic_names = [\"Topic\"+str(i) for i in range(lda_model.n_components)]\n",
    "\n",
    "    lda_output_dataframe = pd.DataFrame(np.round(lda_output, 2), columns=topic_names)\n",
    "\n",
    "    return gs, feature_names, data_bow, vectorizer, lda_output_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_topic(lda_model, data_bow, dataset_row_nb):\n",
    "    lda_output = lda_model.transform(data_bow)\n",
    "\n",
    "    topic_names = [\"Topic\"+str(i) for i in range(lda_model.n_components)]\n",
    "    doc_names = [\"Doc\"+str(i) for i in range(dataset_row_nb)]\n",
    "\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topic_names, index=doc_names)\n",
    "\n",
    "    dominant_topics = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic[\"Dominant_Topic\"] = dominant_topics\n",
    "\n",
    "    return df_document_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_distribution(dominant_topic_df):\n",
    "    distribution = dominant_topic_df[\"Dominant_Topic\"].value_counts().reset_index()\n",
    "    distribution.columns = [\"Dominant_Topic\", \"Count\"]\n",
    "\n",
    "    plt.figure(figsize=(15, 4))\n",
    "\n",
    "    sns.countplot(data=dominant_topic_df, x=\"Dominant_Topic\")\n",
    "\n",
    "    plt.title(\"Tag count\", size=20)\n",
    "    plt.xlabel(\"Tag\", size=16)\n",
    "    plt.ylabel(\"Count\", size=16)\n",
    "    plt.xticks(rotation=45, size=16, ha=\"right\")\n",
    "    plt.yticks(size=16)\n",
    "    plt.show()\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_words(lda_model, feature_names, n_words=20):\n",
    "    keywords = np.array(feature_names)\n",
    "    topic_keywords = []\n",
    "    for topic_weight in lda_model.components_:\n",
    "        topic_keyword_locs = (-topic_weight).argsort()[:n_words]\n",
    "        topic_keywords.append(feature_names.take(topic_keyword_locs))\n",
    "    \n",
    "    topic_keywords_df = pd.DataFrame(data=topic_keywords)\n",
    "    topic_keywords_df.columns = [\"Word\"+str(i) for i in range(topic_keywords_df.shape[1])]\n",
    "    topic_keywords_df.index = [\"Topic\"+str(i) for i in range(topic_keywords_df.shape[0])]\n",
    "    return topic_keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(lda_model, sentence, vectorizer, topic_keywords_dataset):\n",
    "    sentence = scripts.preprocess_sentence(sentence)\n",
    "    data_bow = vectorizer.transform([sentence])\n",
    "    topic_probability_score = lda_model.transform(data_bow)\n",
    "    topic = topic_keywords_dataset.iloc[np.argmax(topic_probability_score), :]\n",
    "    topic_name = topic.name\n",
    "    topic_words = topic.values.tolist()\n",
    "    return topic_name, topic_words, topic_probability_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_tuning_post_lda(dataset_X: pd.DataFrame, dataset_y: pd.DataFrame, meta_model, model, param_grid: dict, scoring: str = \"f1_micro\"):\n",
    "    start = datetime.now()\n",
    "\n",
    "    # target multi label binarizer\n",
    "    multi_label_binarizer = MultiLabelBinarizer()\n",
    "    y = multi_label_binarizer.fit_transform(dataset_y)\n",
    "\n",
    "    feature_names = dataset_X.columns\n",
    "    classes = multi_label_binarizer.classes_\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset_X, y, test_size = 0.33, random_state = 1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # multioutput or onevsrest ...\n",
    "    meta_model.fit(X_train, y_train)\n",
    "\n",
    "    # gridsearch tuning/fitting\n",
    "    gs = GridSearchCV(meta_model, param_grid, scoring=scoring, refit=True)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # advanced evaluation\n",
    "    best_model = gs.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    print(f\"classifier_tuning > Time taken to run this cell : {datetime.now() - start} \\n\")\n",
    "\n",
    "    return gs, classes, y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gs, classes, y_test, y_pred):\n",
    "    start = datetime.now()\n",
    "\n",
    "    print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Hamming loss \", hamming_loss(y_test, y_pred))\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average='micro')\n",
    "    recall = recall_score(y_test, y_pred, average='micro')\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(\"Macro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "    print(classification_report(y_test, y_pred, target_names=classes, zero_division=0))\n",
    "\n",
    "    print(f\"evaluate > Time taken to run this cell : {datetime.now() - start}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.1 Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    param_grid = {\n",
    "        \"n_components\": [10],\n",
    "        \"learning_decay\": [.7],\n",
    "        \"random_state\": [0],\n",
    "        \"n_jobs\": [10]\n",
    "    }\n",
    "\n",
    "    gs, feature_names, data_bow, vectorizer, lda_output_dataframe = latent_dirichlet_allocation_tuning(data[\"Sentence\"], param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    print(f\"Log likelihood: {gs.best_estimator_.score(data_bow)}\")\n",
    "    print(f\"Perplexity: {gs.best_estimator_.perplexity(data_bow)}\")\n",
    "    print(f\"Best params: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    dominant_topic_df = get_dominant_topic(gs.best_estimator_, data_bow, data.shape[0])\n",
    "    dominant_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    topic_distribution(dominant_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    topic_keywords_df = pd.DataFrame(data=gs.best_estimator_.components_, columns=feature_names, index=dominant_topic_df.columns[:-1])\n",
    "    topic_keywords_df.info()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"baseline\"]:\n",
    "\n",
    "    display = topic_keywords_df.iloc[:, :20]\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"baseline\"]:\n",
    "\n",
    "    topic_keywords_dataset = topic_words(lda_model=gs.best_estimator_, feature_names=feature_names, n_words=20)\n",
    "    display = topic_keywords_dataset\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    param_grid = {\n",
    "        \"estimator__solver\": [\"liblinear\"],\n",
    "        \"estimator__penalty\": [\"l1\"],\n",
    "        \"estimator__random_state\": [0],\n",
    "    }\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    meta_model = OneVsRestClassifier(model)\n",
    "    gs, classes, y_test, y_pred = classifier_tuning_post_lda(lda_output_dataframe, data[\"Tags_Reduced\"], meta_model, model, param_grid)\n",
    "    evaluate(gs, classes, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.2 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/lda_results_metrics.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/lda_results_topic_distrib_plot.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/lda_results_topic_words.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/logistic_classifier_results_01.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/logistic_classifier_results_02.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/logistic_classifier_results_03.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 5 NN preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pseudo_list(row):\n",
    "    ls = row[\"Tags_Reduced\"]\n",
    "    cell = \"[\"\n",
    "    for elt in ls:\n",
    "        cell = cell + \"'\" + elt + \"',\"\n",
    "    cell = cell + \"]\"\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def at_least_two(row):\n",
    "    tags_list = row[\"Tags_Reduced\"]\n",
    "    tags_list_len = len(tags_list)\n",
    "    if tags_list_len > 1:\n",
    "        return row[\"Tags_Reduced\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about multilabelbinarizer, reverse action\n",
    "def to_class(mlb, vec):\n",
    "    indexes = [i for i in range(len(vec)) if vec[i] != 0]\n",
    "    return ' '.join([list(mlb.classes_)[i] for i in indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_accuracy(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"For multi-label classification, one has to define a custom\n",
    "    acccuracy function because neither tf.keras.metrics.Accuracy nor\n",
    "    tf.keras.metrics.CategoricalAccuracy evaluate the number of \n",
    "    exact matches.\n",
    "\n",
    "    :Example:\n",
    "    >>> from tensorflow.keras import metrics\n",
    "    >>> y_true = tf.convert_to_tensor([[1., 1.]])\n",
    "    >>> y_pred = tf.convert_to_tensor([[1., 0.]])\n",
    "    >>> metrics.Accuracy()(y_true, y_pred).numpy()\n",
    "    0.5\n",
    "    >>> metrics.CategoricalAccuracy()(y_true, y_pred).numpy()\n",
    "    1.0\n",
    "    >>> multi_label_accuracy(y_true, y_pred).numpy()\n",
    "    0.0\n",
    "    \"\"\"   \n",
    "    y_pred = tf.math.round(y_pred)\n",
    "    exact_matches = tf.math.reduce_all(y_pred == y_true, axis=1)\n",
    "    exact_matches = tf.cast(exact_matches, tf.float32)\n",
    "    return tf.math.reduce_mean(exact_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y_true, y_pred):\n",
    "    return K.mean(y_true*(1-y_pred)+(1-y_true)*y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Custom_Hamming_Loss1(y_true, y_pred):\n",
    "    tmp = K.abs(y_true-y_pred)\n",
    "    return K.mean(K.cast(K.greater(tmp,0.5),dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(y_true, y_pred, smooth=100):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_history(history, figsize=(20, 10), metrics: str = \"categorical_accuracy\"):\n",
    "    fix, axs = plt.subplots(2, 1, figsize=figsize, sharex=True)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title(\"Loss\")\n",
    "    sns.lineplot(data=history, x=history.index, y=\"loss\", label=\"loss\")\n",
    "    sns.lineplot(data=history, x=history.index, y=\"val_loss\", label=\"val_loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.tick_params(labelright=True)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title(\"Accuracy\")\n",
    "    sns.lineplot(data=history, x=history.index, y=metrics, label=metrics)\n",
    "    sns.lineplot(data=history, x=history.index, y=\"val_\" + metrics, label=\"val_\" + metrics)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.tick_params(labelright=True)\n",
    "    plt.legend()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5.1 Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = data[[\"Tags_Reduced\"]]\n",
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[\"Sentence_Pristine\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = MultiLabelBinarizer()\n",
    "y_binarized = binarizer.fit_transform(data[\"Tags_Reduced\"])\n",
    "y_binarized_df = pd.DataFrame(y_binarized, columns=binarizer.classes_, index=data.index)\n",
    "data = X.join(y_binarized_df)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.iloc[:, 1:]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[\"Sentence_Pristine\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Sentence_Pristine\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_test_, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lenghts = [len(t.split()) for t in X]\n",
    "ax = sns.histplot(data=text_lenghts, kde=True, stat=\"density\")\n",
    "ax.set_title(\"Texts length distribution (number of words):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 6 BERT_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Loading BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "    \n",
    "    bert_config = BertConfig.from_pretrained(model_name)\n",
    "    bert_config.output_hidden_states = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path=model_name, config=bert_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6.2 BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "    \n",
    "    X_train = tokenizer(\n",
    "        text=X_train_.to_list(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    X_test = tokenizer(\n",
    "        text=X_test_.to_list(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True, \n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids = True,\n",
    "        return_attention_mask = True,\n",
    "        verbose = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    X_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    X_train[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    X_train[\"token_type_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    X_train[\"attention_mask\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6.3 Tensorflow dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "    \n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\n",
    "    ds_test = tf.data.Dataset.from_tensor_slices((dict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6.4 Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    bert_model_name = \"bert-base-uncased\"\n",
    "    max_seq_len = 100\n",
    "    tags_nb = TAGS_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    input_ids = keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32, name='input_ids')\n",
    "    input_type = keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32, name='token_type_ids')\n",
    "    input_mask = keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32, name='attention_mask')\n",
    "    inputs = [input_ids, input_type, input_mask]\n",
    "\n",
    "    bert = TFBertModel.from_pretrained(bert_model_name)\n",
    "    bert_outputs = bert(inputs)\n",
    "    last_hidden_states = bert_outputs.last_hidden_state\n",
    "\n",
    "    avg = keras.layers.GlobalAveragePooling1D()(last_hidden_states)\n",
    "    output = keras.layers.Dense(tags_nb, activation=\"sigmoid\")(avg)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    max_epochs = 4\n",
    "    batch_size = 4\n",
    "\n",
    "    opt = tfa.optimizers.RectifiedAdam(learning_rate=3e-5)\n",
    "    loss = keras.losses.BinaryCrossentropy()\n",
    "    best_weights_file = \"bert_base_weights.h5\"\n",
    "\n",
    "    m_ckpt = ModelCheckpoint(\n",
    "        best_weights_file,\n",
    "        monitor='val_auc_1',\n",
    "        mode='max',\n",
    "        verbose=2,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=opt,\n",
    "        metrics=[\n",
    "            keras.metrics.AUC(multi_label=True, curve=\"ROC\"),\n",
    "            keras.metrics.BinaryAccuracy()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        ds_train.shuffle(1000).batch(batch_size),\n",
    "        validation_data=ds_test.batch(batch_size),\n",
    "        epochs=max_epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[m_ckpt],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6.5 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    history_bert = pd.DataFrame(data=history.history)\n",
    "    for metric in [\"auc_1\", \"binary_accuracy\"]:\n",
    "        visualize_history(history_bert, metrics=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 6.5.1 History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/bert_base_results_auc.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/bert_base_results_binaccuracy.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 7 BERT_SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Loading BERT_SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    pret_model = pd.read_csv('data/bert_se/BERT_SE.csv', delimiter= ',', header=None)\n",
    "    MAX_LEN = data.shape[0]\n",
    "    print(f\"MAX_LEN: {MAX_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    embedding_matrix = pret_model.iloc[0:MAX_LEN,:]\n",
    "    dfEmbedding_mat = pd.DataFrame(embedding_matrix)\n",
    "    embedding_mat = dfEmbedding_mat.fillna('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7.2 BERT_SE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    bert_config = BertConfig.from_pretrained(model_name)\n",
    "    bert_config.output_hidden_states = False\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path=model_name, config=bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    X_train = tokenizer(\n",
    "        text=X_train_.to_list(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    X_test = tokenizer(\n",
    "        text=X_test_.to_list(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True, \n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids = True,\n",
    "        return_attention_mask = True,\n",
    "        verbose = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    X_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    X_train[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    X_train[\"token_type_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    X_train[\"attention_mask\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7.3 Tensorflow dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\n",
    "    ds_test = tf.data.Dataset.from_tensor_slices((dict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7.4 Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/nlp/multi_label_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    bert_model_name = \"bert-base-uncased\"\n",
    "    max_seq_len = 100\n",
    "    tags_nb = TAGS_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    input_ids = keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32, name='input_ids')\n",
    "    input_type = keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32, name='token_type_ids')\n",
    "    input_mask = keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32, name='attention_mask')\n",
    "    inputs = [input_ids, input_type, input_mask]\n",
    "    \n",
    "    bert = Embedding(MAX_LEN, 768, input_length = 100, name='embedding', trainable=True)\n",
    "    bert.build(input_shape=(1,))\n",
    "    bert.set_weights([embedding_mat])\n",
    "    bert_outputs = bert(input_ids + input_type + input_mask)\n",
    "\n",
    "    avg = keras.layers.GlobalAveragePooling1D()(bert_outputs)\n",
    "\n",
    "    output = keras.layers.Dense(tags_nb, activation=\"sigmoid\")(avg)\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    max_epochs = 4\n",
    "    batch_size = 4 \n",
    "\n",
    "    # opt = tfa.optimizers.RectifiedAdam(learning_rate=3e-5)\n",
    "    opt = Adam(lr = 0.001, beta_1 = 0.99, beta_2 = 0.999, epsilon = None, decay = 0.01, amsgrad = False)\n",
    "    loss = keras.losses.BinaryCrossentropy()\n",
    "    best_weights_file = \"bert_se_weights.h5\"\n",
    "\n",
    "    m_ckpt = ModelCheckpoint(\n",
    "        best_weights_file,\n",
    "        monitor='val_auc_1',\n",
    "        mode='max',\n",
    "        verbose=2,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=opt,\n",
    "        metrics=[\n",
    "            keras.metrics.AUC(multi_label=True, curve=\"ROC\"),\n",
    "            keras.metrics.BinaryAccuracy()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        ds_train.shuffle(1000).batch(batch_size),\n",
    "        validation_data=ds_test.batch(batch_size),\n",
    "        epochs=max_epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[m_ckpt],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7.5 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    history_bert = pd.DataFrame(data=history.history)\n",
    "    for metric in [\"auc\", \"binary_accuracy\"]:\n",
    "        visualize_history(history_bert, metrics=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 8 Conclusion"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "586ad1ed5c97141e2437e681efbf1ec0adcd17d830cf5af2ca3d2819e743e158"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
