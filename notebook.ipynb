{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'trace' from 'tensorflow.python.profiler' (/opt/anaconda/lib/python3.8/site-packages/tensorflow/python/profiler/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2731/3061622666.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_hub/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLatestModuleExporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_module_for_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage_embedding_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_hub/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLatestModuleExporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m   \"\"\"Regularly exports registered modules into timestamped directories.\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdnn_logit_fn_builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmeans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeansClustering\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSDCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanned\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhead_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanned\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_logging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils_impl\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msaved_model_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'trace' from 'tensorflow.python.profiler' (/opt/anaconda/lib/python3.8/site-packages/tensorflow/python/profiler/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os, warnings, re, shutil\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score, hamming_loss, precision_score, recall_score, f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.profiler import trace\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Embedding, AveragePooling1D, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, AUC\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import bert\n",
    "from transformers import TFBertModel, BertConfig, BertTokenizerFast\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "import scripts\n",
    "\n",
    "def set_seed(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed()\n",
    "\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "if tf.test.gpu_device_name():\n",
    "    print(f\"GPU used: {tf.test.gpu_device_name()}\")\n",
    "else:\n",
    "    print(f\"GPU not used\")\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"raw_preprocessing\": False,\n",
    "    \"preprocessing\": False,\n",
    "    \"dataset_constraints\": True,\n",
    "    \"baseline\": False,\n",
    "    \"bert_base\": True,\n",
    "    \"bert_se\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_raw_data_body(cell):\n",
    "    soup = bs(cell, \"html.parser\")\n",
    "\n",
    "    script_tags = soup.find_all(\"script\")\n",
    "    for script_tag in script_tags:\n",
    "        script_tag.extract()\n",
    "\n",
    "    code_tags = soup.find_all(\"code\")\n",
    "    for code_tag in code_tags:\n",
    "        code_tag.extract()\n",
    "\n",
    "    preproc_cell = soup.get_text()\n",
    "    preproc_cell = preproc_cell.replace(',', ' ')\n",
    "\n",
    "    return preproc_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_list(cell):\n",
    "    return [tag for tag in re.split(r'[<>]', cell) if tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(cell):\n",
    "    return cell.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(*texts):\n",
    "    tokens = []\n",
    "    for text in texts:\n",
    "        # https://regex101.com/\n",
    "        tokenizer = nltk.RegexpTokenizer(r'\\.?[a-z#]+')\n",
    "        tokens_temp = tokenizer.tokenize(text)\n",
    "        tokens += [re.sub(\"(.)\\\\1{3,}\", \"\\\\1\", token) for token in tokens_temp]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(cell):\n",
    "    return [word for word in cell if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_pos(cell):\n",
    "    treebank_tags = pos_tag(cell)\n",
    "    pos = [(tag[0], get_wordnet_pos(tag[1])) for tag in treebank_tags]\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(cell, with_pos=False):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    if not with_pos:\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in cell]\n",
    "    else:\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(pair[0], pos=pair[1]) for pair in cell]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmize(cell):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in cell]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data_body = pd.read_csv(\"data/raw_data_body.csv\")\n",
    "    raw_data_body.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Parse html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data_body[\"Body\"] = raw_data_body.apply(lambda row: preproc_raw_data_body(row.Body), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Merge body with rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data_rest = pd.read_csv(\"data/raw_data_id_title_tags.csv\")\n",
    "    raw_data = raw_data_rest.join(raw_data_body)\n",
    "    raw_data = raw_data.set_index(\"Id\").reindex([\"Title\", \"Body\", \"Tags\"], axis=\"columns\")\n",
    "    raw_data[\"Sentence_Pristine\"] = raw_data.apply(lambda row: row[\"Title\"] + \" \" + row[\"Body\"], axis=\"columns\")\n",
    "    display = raw_data.head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Tags to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data[\"Tags\"] = raw_data.apply(lambda row: tags_to_list(row[\"Tags\"]), axis=\"columns\")\n",
    "    display = raw_data[[\"Tags\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Lowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"raw_preprocessing\"]:\n",
    "\n",
    "    raw_data[\"Sentence_Pristine\"] = raw_data.apply(lambda row: lower(row[\"Sentence_Pristine\"]), axis=\"columns\")\n",
    "    display = raw_data[[\"Sentence_Pristine\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Save raw preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"raw_preprocessing\"]:\n",
    "    \n",
    "    raw_data.drop(columns=[\"Title\", \"Body\"], inplace=True)\n",
    "    raw_data.to_csv(\"data/raw_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Load raw preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"preprocessing\"]:\n",
    "    \n",
    "    data = pd.read_csv(\"data/raw_data.csv\", index_col=\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"Tokens\"] = data.apply(lambda row: tokenize(row[\"Sentence_Pristine\"]), axis=\"columns\")\n",
    "    display = data[[\"Sentence_Pristine\", \"Tokens\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 StopWords deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    data[\"Tokens\"] = data.apply(lambda row: remove_stop_words(row[\"Tokens\"]), axis=\"columns\")\n",
    "    display = data[[\"Sentence_Pristine\", \"Tokens\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 POS - Part-Of-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"POS\"] = data.apply(lambda row: tag_pos(row[\"Tokens\"]), axis=\"columns\")\n",
    "    display = data[[\"Tokens\", \"POS\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"Lemmatized\"] = data.apply(lambda row: lemmatize(row[\"POS\"], with_pos=True), axis=\"columns\")\n",
    "    display = data[[\"Tokens\", \"Lemmatized\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 Stemmize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"LemmaAndStem\"] = data.apply(lambda row: stemmize(row[\"Tokens\"]), axis=\"columns\")\n",
    "    display = data[[\"Tokens\", \"LemmaAndStem\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13 Generating sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data[\"Sentence\"] = data.apply(lambda row: \" \".join([str(item) for item in row[\"LemmaAndStem\"]]), axis=\"columns\")\n",
    "    display = data[[\"LemmaAndStem\", \"Sentence\"]].head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.14 Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"preprocessing\"]:\n",
    "\n",
    "    data.to_csv(\"data/data_cleaned.csv\", index_label=\"Id\")\n",
    "    display = data.head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(dataset, max_features=None, min_df=0.0, max_df=1.0):\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features=max_features, max_df=max_df, min_df=min_df)\n",
    "    matrix = vectorizer.fit_transform(dataset)\n",
    "\n",
    "    data_dense = matrix.todense()\n",
    "    print(f\"Sparcity: {((data_dense > 0).sum() / data_dense.size)*100:.4}%\")\n",
    "\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "    matrix = matrix.toarray()\n",
    "    bag = pd.DataFrame(data=matrix, columns=vocab)\n",
    "    return bag, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(dataset, max_features=None, min_df=0.0, max_df=1.0):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=None, stop_words=None, max_features=max_features, min_df=min_df, max_df=max_df)\n",
    "    matrix = vectorizer.fit_transform(dataset).toarray()\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    tfidf = pd.DataFrame(data=matrix, columns=vocab)\n",
    "    return tfidf, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data_cleaned.csv\", index_col=\"Id\")\n",
    "data = data.reset_index()\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "data[\"Tags\"] = data[\"Tags\"].apply(eval)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"Sentence_Pristine\", \"Sentence\", \"Tags\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.2 Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"Tags\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Tags.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "for row in data.Tags.values:\n",
    "    tags += row\n",
    "tags_df = pd.DataFrame(data=tags, columns=[\"Tag\"]).value_counts().reset_index()\n",
    "tags_df.columns = [\"Tag\", \"Count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "sns.barplot(data=tags_df.iloc[:20], x=\"Tag\", y=\"Count\")\n",
    "\n",
    "plt.title(\"Tag count\", size=20)\n",
    "plt.xlabel(\"Tag\", size=16)\n",
    "plt.ylabel(\"Count\", size=16)\n",
    "plt.xticks(rotation=45, size=16, ha=\"right\")\n",
    "plt.yticks(size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "ax = sns.ecdfplot(data=tags_df, x=\"Count\", log_scale=True)\n",
    "\n",
    "plt.axhline(0.98, linestyle=\"--\", linewidth=1, color=\"r\")\n",
    "plt.axvline(100, linestyle=\"--\", linewidth=1, color=\"r\")\n",
    "\n",
    "plt.title(\"Cummulative coverage percentage\", size=20)\n",
    "plt.xlabel(\"Number of post\", size=16)\n",
    "plt.ylabel(\"Proportion\", size=16)\n",
    "plt.xticks(rotation=45, size=16, ha=\"right\")\n",
    "plt.yticks(size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = tags_df[:100]\n",
    "tags_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = dict(zip(tags_df.Tag, tags_df.Count))\n",
    "wordcloud = WordCloud(background_color=\"black\", width=1600, height=800).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags_df.Tag.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_or_remove(cell, word_list):\n",
    "    return [word for word in cell if word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tags_Reduced\"] = data.apply(lambda row: find_or_remove(row[\"Tags\"], tags),axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tags_Reduced\"].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tags_Reduced\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Tags_Reduced\"] = data.apply(lambda row: at_least_two(row), axis=1)\n",
    "# data.dropna(subset=[\"Tags_Reduced\"], inplace=True)\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.3 Dataset constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"dataset_constraints\"]:\n",
    "    \n",
    "    data = data.iloc[:1000]\n",
    "    data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 4 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/#9buildldamodelwithsklearn\n",
    "#\n",
    "def latent_dirichlet_allocation_tuning(dataset: pd.DataFrame, param_grid: dict):\n",
    "    data_bow, vectorizer = bow(dataset, min_df=.005)\n",
    "    feature_names = data_bow.columns\n",
    "\n",
    "    lda = LatentDirichletAllocation()\n",
    "    gs = GridSearchCV(lda, param_grid)\n",
    "    gs.fit(data_bow)\n",
    "\n",
    "    lda_model = gs.best_estimator_\n",
    "    lda_output = lda_model.transform(data_bow)\n",
    "    topic_names = [\"Topic\"+str(i) for i in range(lda_model.n_components)]\n",
    "\n",
    "    lda_output_dataframe = pd.DataFrame(np.round(lda_output, 2), columns=topic_names)\n",
    "\n",
    "    return gs, feature_names, data_bow, vectorizer, lda_output_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_topic(lda_model, data_bow, dataset_row_nb):\n",
    "    lda_output = lda_model.transform(data_bow)\n",
    "\n",
    "    topic_names = [\"Topic\"+str(i) for i in range(lda_model.n_components)]\n",
    "    doc_names = [\"Doc\"+str(i) for i in range(dataset_row_nb)]\n",
    "\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topic_names, index=doc_names)\n",
    "\n",
    "    dominant_topics = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic[\"Dominant_Topic\"] = dominant_topics\n",
    "\n",
    "    return df_document_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_distribution(dominant_topic_df):\n",
    "    distribution = dominant_topic_df[\"Dominant_Topic\"].value_counts().reset_index()\n",
    "    distribution.columns = [\"Dominant_Topic\", \"Count\"]\n",
    "\n",
    "    plt.figure(figsize=(15, 4))\n",
    "\n",
    "    sns.countplot(data=dominant_topic_df, x=\"Dominant_Topic\")\n",
    "\n",
    "    plt.title(\"Tag count\", size=20)\n",
    "    plt.xlabel(\"Tag\", size=16)\n",
    "    plt.ylabel(\"Count\", size=16)\n",
    "    plt.xticks(rotation=45, size=16, ha=\"right\")\n",
    "    plt.yticks(size=16)\n",
    "    plt.show()\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_words(lda_model, feature_names, n_words=20):\n",
    "    keywords = np.array(feature_names)\n",
    "    topic_keywords = []\n",
    "    for topic_weight in lda_model.components_:\n",
    "        topic_keyword_locs = (-topic_weight).argsort()[:n_words]\n",
    "        topic_keywords.append(feature_names.take(topic_keyword_locs))\n",
    "    \n",
    "    topic_keywords_df = pd.DataFrame(data=topic_keywords)\n",
    "    topic_keywords_df.columns = [\"Word\"+str(i) for i in range(topic_keywords_df.shape[1])]\n",
    "    topic_keywords_df.index = [\"Topic\"+str(i) for i in range(topic_keywords_df.shape[0])]\n",
    "    return topic_keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(lda_model, sentence, vectorizer, topic_keywords_dataset):\n",
    "    sentence = scripts.preprocess_sentence(sentence)\n",
    "    data_bow = vectorizer.transform([sentence])\n",
    "    topic_probability_score = lda_model.transform(data_bow)\n",
    "    topic = topic_keywords_dataset.iloc[np.argmax(topic_probability_score), :]\n",
    "    topic_name = topic.name\n",
    "    topic_words = topic.values.tolist()\n",
    "    return topic_name, topic_words, topic_probability_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_tuning_post_lda(dataset_X: pd.DataFrame, dataset_y: pd.DataFrame, meta_model, model, param_grid: dict, scoring: str = \"f1_micro\"):\n",
    "    start = datetime.now()\n",
    "\n",
    "    # target multi label binarizer\n",
    "    multi_label_binarizer = MultiLabelBinarizer()\n",
    "    y = multi_label_binarizer.fit_transform(dataset_y)\n",
    "\n",
    "    feature_names = dataset_X.columns\n",
    "    classes = multi_label_binarizer.classes_\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset_X, y, test_size = 0.33, random_state = 1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # multioutput or onevsrest ...\n",
    "    meta_model.fit(X_train, y_train)\n",
    "\n",
    "    # gridsearch tuning/fitting\n",
    "    gs = GridSearchCV(meta_model, param_grid, scoring=scoring, refit=True)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # advanced evaluation\n",
    "    best_model = gs.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    print(f\"classifier_tuning > Time taken to run this cell : {datetime.now() - start} \\n\")\n",
    "\n",
    "    return gs, classes, y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gs, classes, y_test, y_pred):\n",
    "    start = datetime.now()\n",
    "\n",
    "    print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Hamming loss \", hamming_loss(y_test, y_pred))\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average='micro')\n",
    "    recall = recall_score(y_test, y_pred, average='micro')\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(\"Macro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "    print(classification_report(y_test, y_pred, target_names=classes, zero_division=0))\n",
    "\n",
    "    print(f\"evaluate > Time taken to run this cell : {datetime.now() - start}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.1 Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    param_grid = {\n",
    "        \"n_components\": [10],\n",
    "        \"learning_decay\": [.7],\n",
    "        \"random_state\": [0],\n",
    "        \"n_jobs\": [10]\n",
    "    }\n",
    "\n",
    "    gs, feature_names, data_bow, vectorizer, lda_output_dataframe = latent_dirichlet_allocation_tuning(data[\"Sentence\"], param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    print(f\"Log likelihood: {gs.best_estimator_.score(data_bow)}\")\n",
    "    print(f\"Perplexity: {gs.best_estimator_.perplexity(data_bow)}\")\n",
    "    print(f\"Best params: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    dominant_topic_df = get_dominant_topic(gs.best_estimator_, data_bow, data.shape[0])\n",
    "    dominant_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    topic_distribution(dominant_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    topic_keywords_df = pd.DataFrame(data=gs.best_estimator_.components_, columns=feature_names, index=dominant_topic_df.columns[:-1])\n",
    "    topic_keywords_df.info()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"baseline\"]:\n",
    "\n",
    "    display = topic_keywords_df.iloc[:, :20]\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"baseline\"]:\n",
    "\n",
    "    topic_keywords_dataset = topic_words(lda_model=gs.best_estimator_, feature_names=feature_names, n_words=20)\n",
    "    display = topic_keywords_dataset\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"baseline\"]:\n",
    "\n",
    "    param_grid = {\n",
    "        \"estimator__solver\": [\"liblinear\"],\n",
    "        \"estimator__penalty\": [\"l1\"],\n",
    "        \"estimator__random_state\": [0],\n",
    "    }\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    meta_model = OneVsRestClassifier(model)\n",
    "    gs, classes, y_test, y_pred = classifier_tuning_post_lda(lda_output_dataframe, data[\"Tags_Reduced\"], meta_model, model, param_grid)\n",
    "    evaluate(gs, classes, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.2 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/lda_results_metrics.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/lda_results_topic_distrib_plot.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/lda_results_topic_words.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/logistic_classifier_results_01.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/logistic_classifier_results_02.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"records/logistic_classifier_results_03.png\" style=\"background-color:white\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 5 NN preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pseudo_list(row):\n",
    "    ls = row[\"Tags_Reduced\"]\n",
    "    cell = \"[\"\n",
    "    for elt in ls:\n",
    "        cell = cell + \"'\" + elt + \"',\"\n",
    "    cell = cell + \"]\"\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def at_least_two(row):\n",
    "    tags_list = row[\"Tags_Reduced\"]\n",
    "    tags_list_len = len(tags_list)\n",
    "    if tags_list_len > 1:\n",
    "        return row[\"Tags_Reduced\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about multilabelbinarizer, reverse action\n",
    "def to_class(mlb, vec):\n",
    "    indexes = [i for i in range(len(vec)) if vec[i] != 0]\n",
    "    return ' '.join([list(mlb.classes_)[i] for i in indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_accuracy(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"For multi-label classification, one has to define a custom\n",
    "    acccuracy function because neither tf.keras.metrics.Accuracy nor\n",
    "    tf.keras.metrics.CategoricalAccuracy evaluate the number of \n",
    "    exact matches.\n",
    "\n",
    "    :Example:\n",
    "    >>> from tensorflow.keras import metrics\n",
    "    >>> y_true = tf.convert_to_tensor([[1., 1.]])\n",
    "    >>> y_pred = tf.convert_to_tensor([[1., 0.]])\n",
    "    >>> metrics.Accuracy()(y_true, y_pred).numpy()\n",
    "    0.5\n",
    "    >>> metrics.CategoricalAccuracy()(y_true, y_pred).numpy()\n",
    "    1.0\n",
    "    >>> multi_label_accuracy(y_true, y_pred).numpy()\n",
    "    0.0\n",
    "    \"\"\"   \n",
    "    y_pred = tf.math.round(y_pred)\n",
    "    exact_matches = tf.math.reduce_all(y_pred == y_true, axis=1)\n",
    "    exact_matches = tf.cast(exact_matches, tf.float32)\n",
    "    return tf.math.reduce_mean(exact_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_history(history, figsize=(20, 10), metrics: str = \"categorical_accuracy\"):\n",
    "    fix, axs = plt.subplots(2, 1, figsize=figsize, sharex=True)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title(\"Loss\")\n",
    "    sns.lineplot(data=history, x=history.index, y=\"loss\", label=\"loss\")\n",
    "    sns.lineplot(data=history, x=history.index, y=\"val_loss\", label=\"val_loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.tick_params(labelright=True)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title(\"Accuracy\")\n",
    "    sns.lineplot(data=history, x=history.index, y=metrics, label=metrics)\n",
    "    sns.lineplot(data=history, x=history.index, y=\"val_\" + metrics, label=\"val_\" + metrics)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.tick_params(labelright=True)\n",
    "    plt.legend()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5.1 Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_, ds_test_ = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lenghts = [len(t.split()) for t in data[\"Sentence_Pristine\"]]\n",
    "ax = sns.histplot(data=text_lenghts, kde=True, stat=\"density\")\n",
    "ax.set_title(\"Texts length distribution (number of words):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 6 BERT_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Loading BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "    \n",
    "    config = BertConfig.from_pretrained(model_name)\n",
    "    config.output_hidden_states = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path=model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "    \n",
    "    transformer_model = TFBertModel.from_pretrained(pretrained_model_name_or_path=model_name, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6.2 BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "    \n",
    "    X_train = tokenizer(\n",
    "        text=ds_train_[\"Sentence_Pristine\"].to_list(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    X_test = tokenizer(\n",
    "        text=ds_test_[\"Sentence_Pristine\"].to_list(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True, \n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids = False,\n",
    "        return_attention_mask = True,\n",
    "        verbose = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "    \n",
    "    binarizer = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "    \n",
    "    y_train = binarizer.fit_transform(ds_train_[\"Tags_Reduced\"])\n",
    "    y_test = binarizer.transform(ds_test_[\"Tags_Reduced\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "    \n",
    "    y_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6.3 Tensorflow dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "    \n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\n",
    "    ds_test = tf.data.Dataset.from_tensor_slices((dict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6.4 Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/nlp/multi_label_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "    \n",
    "    bert = transformer_model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    bert = transformer_model.layers[0]\n",
    "    \n",
    "    # input_ids = keras.layers.Input(shape=(max_length,), name=\"input_ids\", dtype=\"int32\")\n",
    "    # inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "    # bert_model = bert(inputs)[0][:, 0, :]\n",
    "    # dropout = keras.layers.Dropout(config.hidden_dropout_prob, name=\"pooled_output\")\n",
    "    # pooled_output = dropout(bert_model, training=False)\n",
    "\n",
    "    # output = keras.layers.Dense(\n",
    "    #     units=y_train.shape[1],\n",
    "    #     kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "    #     activation=\"sigmoid\",\n",
    "    #     name=\"output\"\n",
    "    # )(pooled_output)\n",
    "\n",
    "    # model = Model(inputs=inputs, outputs=output, name='BERT_MultiLabel_MultiClass')\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Input(\n",
    "            shape=(max_length)\n",
    "        )\n",
    "    )\n",
    "    # model.add(bert_base model)\n",
    "    model.add(\n",
    "        transformer_model.layers[0]\n",
    "    )\n",
    "    model.add(\n",
    "        AveragePooling1D(\n",
    "            pool_size=100\n",
    "        )\n",
    "    )\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu')) \n",
    "    model.add(Dense(y_train.shape[1], activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        './bert_base.model',\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        save_weights_only=False,\n",
    "        save_freq=1\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0.001,\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    optimizer = Adam(\n",
    "        learning_rate=5e-05,\n",
    "        epsilon=1e-08,\n",
    "        decay=0.01,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "\n",
    "    # adam = Adam(5e-5)\n",
    "    loss = BinaryCrossentropy()\n",
    "    # loss = CategoricalCrossentropy(from_logits = True)\n",
    "    metric_accuracy = CategoricalAccuracy('accuracy')\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=loss,\n",
    "        metrics=[\n",
    "            metric_accuracy,\n",
    "            multi_label_accuracy,\n",
    "            \"binary_accuracy\",\n",
    "            AUC(name=\"average_precision\", curve=\"PR\", multi_label=True)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "    \n",
    "    EPOCHS = 20\n",
    "\n",
    "    history = model.fit(\n",
    "        ds_train.shuffle(1000).batch(4),\n",
    "        validation_data=ds_test.batch(4),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=4,\n",
    "        callbacks=[early_stopping_callback],\n",
    "        workers=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6.5 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_base\"]:\n",
    "\n",
    "    history_bert = pd.DataFrame(data=history.history)\n",
    "    for metric in [\"multi_label_accuracy\", \"binary_accuracy\", \"average_precision\"]:\n",
    "        visualize_history(history_bert, metrics=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 7 BERT_SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Loading BERT_SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    pret_model = pd.read_csv('data/bert_se//BERT_SE.csv', delimiter= ',', header=None)\n",
    "    MAX_LEN = data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    embedding_matrix = pret_model.iloc[0:1000,:]\n",
    "    dfEmbedding_mat = pd.DataFrame(embedding_matrix)\n",
    "    embedding_mat = dfEmbedding_mat.fillna('0')\n",
    "\n",
    "    embedding = Embedding(MAX_LEN, 768, input_length = 100, name='embedding', trainable=True)\n",
    "    embedding.build(input_shape=(1,))\n",
    "    embedding.set_weights([embedding_mat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7.2 BERT_SE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_LEN, char_level=False, lower=False) \n",
    "    tokenizer.fit_on_texts(ds_train_[\"Sentence_Pristine\"])                            \n",
    "    encSequences = tokenizer.texts_to_sequences(ds_train_[\"Sentence_Pristine\"])          \n",
    "    encSequences_test = tokenizer.texts_to_sequences(ds_test_[\"Sentence_Pristine\"])\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "    X_train = pad_sequences(encSequences, maxlen= MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    X_test = pad_sequences(encSequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    binarizer = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    y_train = binarizer.fit_transform(ds_train_[\"Tags_Reduced\"])\n",
    "    y_test = binarizer.transform(ds_test_[\"Tags_Reduced\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "    \n",
    "    y_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7.3 Tensorflow dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config[\"bert_se\"]:\n",
    "\n",
    "    # ds_train = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\n",
    "    # ds_test = tf.data.Dataset.from_tensor_slices((dict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7.4 Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/nlp/multi_label_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(\n",
    "        AveragePooling1D(\n",
    "            pool_size=100\n",
    "        )\n",
    "    )\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu')) \n",
    "    model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        './bert_base.model',\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        save_weights_only=False,\n",
    "        save_freq=1\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0.001,\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    optimizer = Adam(\n",
    "        learning_rate=5e-05,\n",
    "        epsilon=1e-08,\n",
    "        decay=0.01,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "\n",
    "    # adam = Adam(5e-5)\n",
    "    loss = BinaryCrossentropy()\n",
    "    # loss = CategoricalCrossentropy(from_logits = True)\n",
    "    metric_accuracy = CategoricalAccuracy('accuracy')\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=loss,\n",
    "        metrics=[\n",
    "            metric_accuracy,\n",
    "            multi_label_accuracy,\n",
    "            \"binary_accuracy\",\n",
    "            AUC(name=\"average_precision\", curve=\"PR\", multi_label=True)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    EPOCHS = 20\n",
    "\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=4,\n",
    "        callbacks=[early_stopping_callback],\n",
    "        workers=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7.5 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"bert_se\"]:\n",
    "\n",
    "    history_bert = pd.DataFrame(data=history.history)\n",
    "    for metric in [\"multi_label_accuracy\", \"binary_accuracy\", \"average_precision\"]:\n",
    "        visualize_history(history_bert, metrics=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 8 Conclusion"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "586ad1ed5c97141e2437e681efbf1ec0adcd17d830cf5af2ca3d2819e743e158"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
